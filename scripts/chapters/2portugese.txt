
Capítulo 2. Classificação de texto

Agora imagine que você é um cientista de dados que precisa construir um sistema que possa automaticamente
identificar estados emocionais como “raiva” ou “alegria” que as pessoas expressam em relação à sua empresa
produto no Twitter. Até 2018, a abordagem de aprendizado profundo para esse problema normalmente envolvia
encontrar uma arquitetura neural adequada para a tarefa e treiná-la do zero em um conjunto de dados de
tweets rotulados. Essa abordagem sofria de três grandes desvantagens:
Você precisava de muitos dados rotulados para treinar modelos precisos, como recorrentes ou
redes neurais convolucionais.
O treinamento desses modelos a partir do zero era demorado e caro.

O modelo treinado não pôde ser facilmente adaptado a uma nova tarefa, por exemplo, com um conjunto diferente de
rótulos.
Hoje em dia, essas limitações são amplamente superadas por meio do aprendizado por transferência, onde normalmente um
A arquitetura baseada em transformador é pré-treinada em uma tarefa genérica, como modelagem de linguagem e
então reutilizado para uma ampla variedade de tarefas downstream. Embora o pré-treinamento de um Transformer possa
envolvem dados significativos e recursos de computação, muitos desses modelos de linguagem são feitos
disponível gratuitamente por grandes laboratórios de pesquisa e pode ser facilmente baixado do Hugging Face
Central Modelo!
Este capítulo irá guiá-lo através de várias abordagens para a detecção de emoções usando um famoso
Modelo de transformador chamado BERT, abreviação de representações de codificador bidirecional de
Transformers.1 Este será nosso primeiro encontro com as três bibliotecas principais do Hugging
Ecossistema facial: conjuntos de dados, tokenizadores e transformadores. Como mostrado na Figura 2-2, esses
bibliotecas nos permitirão passar rapidamente do texto bruto para um modelo ajustado que pode ser usado para
inferência em novos tweets. Então, no espírito do Optimus Prime, vamos mergulhar, “transformar e
sair da cama!"

O conjunto de dados
Para construir nosso detector de emoções, usaremos um ótimo conjunto de dados de um artigo2 que explorou como
as emoções são representadas nas mensagens do Twitter em inglês. Ao contrário da maioria dos conjuntos de dados de análise de sentimento
que envolvem apenas polaridades “positivas” e “negativas”, este conjunto de dados contém seis emoções básicas:
raiva, nojo, medo, alegria, tristeza e surpresa. Dado um tweet, nossa tarefa será treinar um modelo
que pode classificá-lo em uma dessas emoções!

Uma primeira olhada nos conjuntos de dados de rostos abraçados
Usaremos a biblioteca Hugging Face Datasets para baixar os dados do Hugging Face
Hub do conjunto de dados. Esta biblioteca foi projetada para carregar e processar grandes conjuntos de dados com eficiência, compartilhá-los
com a comunidade e simplificar a interoperabilidade entre NumPy, Pandas, PyTorch e
TensorFlow. Ele também contém muitos conjuntos de dados e métricas de benchmark de NLP, como o Stanford
Conjunto de dados de resposta a perguntas (SQuAD), avaliação geral de compreensão de linguagem (GLUE),
e Wikipédia. Podemos usar a função list_datasets para ver quais conjuntos de dados estão disponíveis
no Hub.

Isso se parece com o conjunto de dados que procuramos, então, em seguida, podemos carregá-lo com o load_dataset
função de conjuntos de dados.

Em cada caso, a estrutura de dados resultante depende do tipo de consulta; embora isso possa parecer
estranho a princípio, é parte do molho secreto que torna os conjuntos de dados tão flexíveis!
Então, agora que vimos como carregar e inspecionar dados com conjuntos de dados, vamos fazer algumas análises
verifica o conteúdo de nossos tweets.

De conjuntos de dados a quadros de dados
Embora os conjuntos de dados forneçam muitas funcionalidades de baixo nível para dividir e dividir nossos dados, muitas vezes é
conveniente para converter um objeto Dataset em um Pandas DataFrame para que possamos acessar APIs de alto nível para visualização de dados. Para habilitar a conversão, Datasets fornece um
Função Dataset.set_format que nos permite alterar o formato de saída do Dataset.
Isso não altera o formato de dados subjacente que é o Apache Arrow e você pode alternar para
outro formato mais tarde, se necessário.

Como podemos ver, os cabeçalhos das colunas foram preservados e as primeiras linhas correspondem ao nosso
visualizações anteriores dos dados.

Antes de mergulhar na construção de um classificador, vamos dar uma olhada no conjunto de dados. como Andrej
Karpathy disse de forma famosa, tornar-se “um com os dados”3 é essencial para a construção de grandes modelos.

Observe a distribuição de classes

Sempre que você estiver trabalhando em problemas de classificação de texto, é uma boa ideia examinar os
distribuição de exemplos entre cada classe. Por exemplo, um conjunto de dados com uma classe distorcida
distribuição pode exigir um tratamento diferente em termos de perda de treinamento e avaliação
métricas do que uma equilibrada.

Podemos ver que o conjunto de dados está fortemente desequilibrado; as aulas de alegria e tristeza aparecem
freqüentemente, enquanto o amor e a tristeza são cerca de 5 a 10 vezes mais raros. Existem várias maneiras de
lidar com dados desequilibrados, como reamostragem das classes minoritárias ou majoritárias. Alternativamente,
também podemos ponderar a função de perda para contabilizar as classes sub-representadas. No entanto, para
manter as coisas simples nesta primeira aplicação prática deixamos estas técnicas como um exercício para
o leitor e passar a examinar o comprimento de nossos tweets.

Quanto tempo duram nossos tweets?
Os modelos de transformador têm um comprimento máximo de sequência de entrada que é referido como o máximo
tamanho do contexto. Para a maioria dos aplicativos com BERT, o tamanho máximo do contexto é de 512 tokens, onde
um token é definido pela escolha do tokenizador e pode ser uma palavra, subpalavra ou caractere. vamos
faça uma estimativa aproximada de nossos comprimentos de tweet por emoção, observando a distribuição de palavras
por tweet.

A partir do enredo, vemos que, para cada emoção, a maioria dos tweets tem cerca de 15 palavras e o
os tweets mais longos estão bem abaixo do tamanho máximo de contexto do BERT de 512 tokens. Textos que são
mais longo do que a janela de contexto de um modelo precisa ser truncado, o que pode levar a uma perda de
desempenho se o texto truncado contiver informações cruciais. Vamos agora descobrir como podemos
converta esses textos brutos em um formato adequado para Transformers!

Do texto aos tokens
Modelos de transformador como BERT não podem receber strings brutas como entrada; em vez disso, eles assumem o
o texto foi tokenizado em vetores numéricos. A tokenização é a etapa de quebrar um
string nas unidades atômicas usadas no modelo. Existem várias estratégias de tokenização que podem ser
adotar e a divisão ideal de palavras em subunidades geralmente é aprendida a partir do corpus. Antes
olhando para o tokenizer usado para BERT, vamos motivá-lo olhando para dois casos extremos:
tokenizadores de caracteres e palavras.

Tokenização de personagem
O esquema de tokenização mais simples é alimentar cada caractere individualmente para o modelo. Em
Python, objetos str são realmente arrays sob o capô, o que nos permite implementar rapidamente
tokenização em nível de caractere com apenas uma linha de código.

Este é um bom começo, mas ainda não terminamos porque nosso modelo espera que cada personagem seja
convertido para um número inteiro, um processo chamado de numeração.

Estamos quase terminando! Cada token foi mapeado para um identificador numérico exclusivo, portanto, o
nome input_ids. A última etapa é converter input_ids em um tensor 2d de vetores one-hot
que são mais adequados para redes neurais do que a representação categórica de input_ids.
A razão para isso é que os elementos de input_ids criam uma escala ordinal, então adicionar ou
subtrair dois IDs é uma operação sem sentido, pois o resultado em um novo ID que representa
outro token aleatório. Por outro lado, o resultado da adição de duas codificações one-hot pode ser
facilmente interpretado: as duas entradas que são “quentes” indicam que os dois tokens correspondentes ocorrem simultaneamente.

Em nosso exemplo simples, podemos ver que a tokenização em nível de caractere ignora qualquer estrutura em
os textos como palavras e os trata apenas como fluxos de caracteres. Embora isso ajude a lidar
com erros ortográficos e palavras raras, a principal desvantagem é que estruturas linguísticas como palavras
precisam ser aprendidos, e esse processo requer computação e memória significativas. Por esta razão,
a tokenização de caracteres raramente é usada na prática. Em vez disso, alguma estrutura do texto, como
palavras é preservada durante a etapa de tokenização. A tokenização de palavras é um processo direto
abordagem para conseguir isso - vamos dar uma olhada em como isso funciona!

Palavra Tokenização
Em vez de dividir o texto em caracteres, podemos dividi-lo em palavras e mapear cada palavra para um
inteiro. Ao usar palavras desde o início, o modelo pode pular a etapa de aprender palavras de
personagens e, assim, eliminar a complexidade do processo de treinamento.

A partir daqui, podemos seguir os mesmos passos que demos para o tokenizador de caracteres e mapear cada palavra
a um identificador único. No entanto, já podemos ver um problema potencial com isso
esquema de tokenização; pontuação não é contabilizada, então NLP. é tratado como um único token.
Dado que as palavras podem incluir declinações, conjugações ou erros ortográficos, o tamanho do
vocabulário pode facilmente crescer para milhões!

OBSERVAÇÃO
Existem variações de tokenizadores de palavras que possuem regras extras para pontuação. Pode-se também aplicar stemming que
normaliza as palavras para sua raiz (por exemplo, “grande”, “maior” e “maior” todos se tornam “grande”) às custas de
perder algumas informações no texto.

A razão pela qual ter um grande vocabulário é um problema é que requer redes neurais com
um número enorme de parâmetros. Para ilustrar isso, suponha que tenhamos 1 milhão de palavras únicas
e deseja comprimir os vetores de entrada de 1 milhão de dimensões para 1 mil dimensões
vetores na primeira camada de uma rede neural. Este é um passo padrão na maioria das arquiteturas NLP
e a matriz peso resultante desse vetor conteria 1 milhão × 1 mil pesos =
1 bilhão de pesos. Isso já é comparável ao maior modelo GPT-2 que tem 1,4 bilhão
parâmetros no total!
Naturalmente, queremos evitar tanto desperdício com os parâmetros de nosso modelo, pois eles são
caros para treinar e modelos maiores são mais difíceis de manter. Uma abordagem comum é
limite o vocabulário e descarte palavras raras considerando, digamos, as 100.000 mais comuns

palavras no corpus. As palavras que não fazem parte do vocabulário são classificadas como “desconhecidas” e
mapeado para um token UNK compartilhado. Isso significa que perdemos algumas informações potencialmente importantes
no processo de tokenização de palavras, pois o modelo não tem informações sobre quais palavras
foram associados aos tokens UNK.
Não seria bom se houvesse um compromisso entre tokenização de caracteres e palavras que
preserva todas as informações de entrada e parte da estrutura de entrada? Há! Vejamos os principais
ideias por trás da tokenização de subpalavras.

Tokenização de subpalavra
A ideia por trás da tokenização de subpalavras é tirar o melhor dos dois mundos de caráter e
tokenização de palavras. Por um lado, queremos usar caracteres, pois eles permitem que o modelo lide
com raras combinações de caracteres e erros ortográficos. Por outro lado, queremos manter frequentes
palavras e partes de palavras como entidades únicas.
AVISO
Alterar a tokenização de um modelo após o pré-treinamento seria catastrófico, pois a palavra e a subpalavra aprendidas
representações se tornariam obsoletas! A biblioteca Transformers fornece funções para garantir que o
tokenizer é carregado para o Transformer correspondente.

Existem vários algoritmos de tokenização de subpalavra, como Byte-Pair-Encoding, WordPiece,
Unigram e SentencePiece. A maioria deles adota uma estratégia semelhante:
Tokenização simples
O corpus de texto é dividido em palavras, geralmente de acordo com as regras de espaço em branco e pontuação.
contando
Todas as palavras do corpus são contadas e a contagem é armazenada.
Dividindo
As palavras na contagem são divididas em subpalavras. Inicialmente, esses são personagens.
Contagem de pares de subpalavras
Usando a contagem, os pares de subpalavras são contados.
Mesclando
Com base em uma regra, alguns dos pares de subpalavras são mesclados no corpus.
Parando
O processo é interrompido quando um tamanho de vocabulário predefinido é atingido.

Existem diversas variações deste procedimento nos algoritmos acima e no Tokenizer
O resumo na documentação do Transformers fornece informações detalhadas sobre cada
estratégia de tokenização. A principal característica distintiva da tokenização de subpalavra (assim como da palavra
tokenização) é que ele é aprendido a partir do corpus usado para pré-treinamento. Vamos dar uma olhada em como
a tokenização de subpalavra realmente funciona usando a biblioteca Hugging Face Transformers!

Usando tokenizadores pré-treinados
Observamos que carregar o tokenizer pré-treinado correto para um determinado modelo pré-treinado é crucial
para obter resultados sensatos. A biblioteca Transformers fornece um conveniente
função from_pretrained que pode ser usada para carregar ambos os objetos, seja do Hugging
Face Model Hub ou de um caminho local.
Para construir nosso detector de emoções, usaremos uma variante do BERT chamada DistilBERT,4 que é um
versão reduzida do modelo BERT original. A principal vantagem deste modelo é que
atinge desempenho comparável ao BERT, sendo significativamente menor e mais
eficiente. Isso nos permite treinar um modelo em poucos minutos e se você quiser treinar um modelo maior
modelo BERT, você pode simplesmente alterar o model_name do modelo pré-treinado. A interface
do modelo e o tokenizer serão os mesmos, o que destaca a flexibilidade do
Biblioteca de transformadores; podemos experimentar uma ampla variedade de modelos de transformadores apenas
alterando o nome do modelo pré-treinado no código!
DICA
É uma boa ideia começar com um modelo menor para que você possa construir rapidamente um protótipo funcional. Assim que você estiver
confiante de que o pipeline está funcionando de ponta a ponta, você pode experimentar modelos maiores para obter ganhos de desempenho.

onde a classe AutoTokenizer garante que emparelhemos o tokenizer e o vocabulário corretos com
a arquitetura modelo.

Podemos examinar alguns atributos do tokenizador, como o tamanho do vocabulário:


Podemos observar duas coisas. Primeiro, os tokens [CLS] e [SEP] foram adicionados
automaticamente para o início e o fim da sequência e, segundo, a palavra longa
complicadotest foi dividido em dois tokens. O prefixo ## em ##test significa que
a string anterior não é um espaço em branco e deve ser mesclada com o token anterior.
Agora que temos uma compreensão básica do processo de tokenização, podemos usar o tokenizador para
alimentar tweets para o modelo.

Treinando um classificador de texto
Conforme discutido no Capítulo 2, os modelos BERT são pré-treinados para prever palavras mascaradas em uma sequência
de texto. No entanto, não podemos usar esses modelos de linguagem diretamente para classificação de texto.
precisamos modificá-los ligeiramente. Para entender quais modificações são necessárias, vamos revisitar
a arquitetura BERT representada na Figura 2-3.

Primeiro, o texto é tokenizado e representado como vetores one-hot cuja dimensão é o tamanho de
o vocabulário do tokenizer, geralmente consistindo de 50k-100k tokens únicos. Em seguida, esses tokens
as codificações são incorporadas em dimensões inferiores e passadas pelas camadas do bloco do codificador para
produz um estado oculto para cada token de entrada. Para o objetivo de pré-treinamento de modelagem de linguagem,
cada estado oculto é conectado a uma camada que prevê o token para o token de entrada, que é
apenas não trivial se o token de entrada foi mascarado. Para a tarefa de classificação, substituímos o
camada de modelagem de linguagem com uma camada de classificação. As sequências BERT sempre começam com um
token de classificação [CLS], portanto, usamos o estado oculto para o token de classificação como
entrada para nossa camada de classificação.

OBSERVAÇÃO
Na prática, o PyTorch pula a etapa de criação de um vetor one-hot porque multiplicar uma matriz por um vetor one-hot
é o mesmo que extrair uma coluna da matriz de incorporação. Isso pode ser feito diretamente obtendo a coluna
com o ID do token da matriz.

Temos duas opções para treinar tal modelo em nosso conjunto de dados do Twitter:

Extração de recursos
Usamos os estados ocultos como recursos e apenas treinamos um classificador neles.
Afinação
Treinamos todo o modelo de ponta a ponta, que também atualiza os parâmetros do pré-treinado
Modelo BERT.
Nesta seção, exploramos as duas opções para o DistilBert e examinamos suas compensações.

Transformadores como extratores de recursos
Usar um Transformer como um extrator de recursos é bastante simples; conforme mostrado na Figura 2-4, congelamos
os pesos do corpo durante o treinamento e usar os estados ocultos como recursos para o classificador. O
A vantagem dessa abordagem é que podemos treinar rapidamente um modelo pequeno ou raso. tal modelo
pode ser uma camada de classificação neural ou um método que não depende de gradientes como
Floresta Aleatória. Este método é especialmente conveniente se as GPUs estiverem indisponíveis, pois o oculto
estados podem ser computados relativamente rápido em uma CPU.

Figura 2-4. Na abordagem baseada em características, o modelo BERT é congelado e apenas fornece características para um classificador.

O método baseado em características baseia-se na suposição de que os estados ocultos capturam todos os
informações necessárias para a tarefa de classificação. No entanto, se alguma informação não for necessária
para a tarefa de pré-treinamento, pode não ser codificado no estado oculto, mesmo que seja crucial
para a tarefa de classificação. Neste caso, o modelo de classificação tem que trabalhar com subótimo
dados, e é melhor usar a abordagem de ajuste fino discutida na seção a seguir.

Aqui, usamos o PyTorch para verificar se uma GPU está disponível e, em seguida, encadeamos o PyTorch
método nn.Module.to("cuda") para o carregador de modelo; sem isso, executaríamos o
modelo na CPU, que pode ser consideravelmente mais lento.
A classe AutoModel corresponde ao codificador de entrada que traduz os vetores one-hot para
embeddings com codificações posicionais e os alimenta através da pilha do codificador para retornar o
estados ocultos. A cabeça do modelo de linguagem que pega os estados ocultos e os decodifica para o
a previsão de token mascarado é excluída, pois é necessária apenas para pré-treinamento. Se você quiser usar
nessa cabeça de modelo, você pode carregar o modelo completo com AutoModelForMaskedLM.

Agora podemos passar esse tensor para o modelo para extrair os estados ocultos. Dependendo do modelo
configuração, a saída pode conter vários objetos, como estados ocultos, perdas ou
atenções, que são arranjadas em uma classe que é semelhante a um namedtuple em Python. Na nossa
Por exemplo, a saída do modelo é uma classe de dados Python chamada BaseModelOutput e, como qualquer
classe, podemos acessar os atributos pelo nome. Como o modelo atual retorna apenas uma entrada
que é o último estado oculto, vamos passar o texto codificado e examinar as saídas:

Olhando para o tensor de estado oculto, vemos que ele tem a forma [batch_size, n_tokens,
hidden_dim]. A maneira como o BERT funciona é que um estado oculto é retornado para cada entrada e o
O modelo usa esses estados ocultos para prever tokens mascarados na tarefa de pré-treinamento. Para
tarefas de classificação, é prática comum usar o estado oculto associado ao [CLS]
token como o recurso de entrada, localizado na primeira posição na segunda dimensão.

Tokenizando todo o conjunto de dados
Agora que sabemos como extrair os estados ocultos de uma única string, vamos tokenizar todo
conjunto de dados! Para fazer isso, podemos escrever uma função simples que irá tokenizar nossos exemplos.

vemos que o resultado é um dicionário, onde cada valor é uma lista de listas geradas pelo
tokenizer. Em particular, cada sequência em input_ids começa com 101 e termina com 102,
seguido de zeros, correspondendo aos tokens [CLS], [SEP] e [PAD] respectivamente:

Observe também que, além de retornar os tweets codificados como input_ids, o tokenizador também
retorna uma lista de arrays de Attention_mask. Isso ocorre porque não queremos que o modelo obtenha
confundido pelos tokens de preenchimento adicionais, então a máscara de atenção permite que o modelo ignore o
partes acolchoadas da entrada. Consulte a Figura 2-5 para obter uma explicação visual sobre como os IDs de entrada e
máscaras de atenção são formatadas.

AVISO
Como os tensores de entrada são empilhados apenas ao serem passados ​​para o modelo, é importante que o tamanho do lote do
correspondência de tokenização e treinamento e que não há embaralhamento. Caso contrário, os tensores de entrada podem não ser empilhados
porque têm comprimentos diferentes. Isso acontece porque eles são preenchidos até o comprimento máximo do
lote de tokenização que pode ser diferente para cada lote. Em caso de dúvida, defina batch_size=None no
etapa de tokenização, pois isso aplicará a tokenização globalmente e todos os tensores de entrada terão o mesmo comprimento.
Isso, no entanto, usará mais memória. Apresentaremos uma alternativa a essa abordagem com uma função de agrupamento
que apenas une os tensores quando eles são necessários e os preenche de acordo.

Para aplicar nossa função tokenize a todo o corpus de emoções, usaremos o
Função DatasetDict.map. Isso aplicará o tokenize em todas as divisões do corpus,
então nossos dados de treinamento, validação e teste serão pré-processados ​​em uma única linha de código:
emoções_encoded = emoções.map(tokenize, batched=Verdadeiro, batch_size=Nenhum)

Por padrão, DatasetDict.map opera individualmente em cada exemplo do corpus, então
a configuração batched=True irá codificar os tweets em lotes, enquanto batch_size=None
aplica nossa função tokenize em um único lote e garante que os tensores de entrada e
as máscaras de atenção têm a mesma forma globalmente. Podemos ver que esta operação adicionou dois
novos recursos para o conjunto de dados: input_ids e a máscara de atenção.

De IDs de entrada a estados ocultos
Agora que convertemos nossos tweets em entradas numéricas, o próximo passo é extrair o último
estados ocultos para que possamos alimentá-los para um classificador. Se tivéssemos um único exemplo, poderíamos
simplesmente passe os input_ids e o Attention_mask para o modelo da seguinte forma

mas o que realmente queremos são os estados ocultos em todo o conjunto de dados. Para isso, podemos usar o
Função DatasetDict.map novamente! Vamos definir uma função forward_pass que leva um
lote de IDs de entrada e máscaras de atenção, os alimenta no modelo e adiciona um novo
hidden_state para nosso lote.

Criando uma Matriz de Recursos
O conjunto de dados pré-processado agora contém todas as informações necessárias para treinar um classificador nele. Nós
usará os estados ocultos como recursos de entrada e os rótulos como alvos. Podemos facilmente criar o
arrays correspondentes no conhecido formato Scikit-Learn como segue.

Redução de Dimensionalidade com UMAP
Antes de treinarmos um modelo nos estados ocultos, é uma boa prática realizar uma verificação de sanidade que
eles fornecem uma representação útil das emoções que queremos classificar. Desde a visualização do
estados ocultos em 768 dimensões é complicado para dizer o mínimo, usaremos o poderoso UMAP5
algoritmo para projetar os vetores em 2D. Como o UMAP funciona melhor quando os recursos são
dimensionado para ficar no intervalo [0,1], primeiro aplicaremos um MinMaxScaler e depois usaremos o UMAP para
reduzir os estados ocultos.

O resultado é uma matriz com o mesmo número de amostras de treinamento, mas com apenas 2 recursos
em vez do 768 com o qual começamos! Vamos investigar os dados compactados um pouco mais e
plote a densidade de pontos para cada categoria separadamente.

OBSERVAÇÃO
Estas são apenas projeções em um espaço dimensional inferior. Só porque algumas categorias se sobrepõem não significa
que eles não são separáveis ​​no espaço original. Inversamente, se forem separáveis ​​no espaço projetado,
ser separável no espaço original.

Agora parece haver padrões mais claros; sentimentos negativos como tristeza, raiva e
medo todos ocupam regiões semelhantes com distribuições ligeiramente variáveis. Por outro lado, alegria
e o amor estão bem separados das emoções negativas e também compartilham um espaço semelhante.
Finalmente, a surpresa está espalhada por todo o lugar. Esperávamos alguma separação, mas isso em nenhum
maneira garantida já que o modelo não foi treinado para saber a diferença entre essas emoções
mas os aprendeu implicitamente, prevendo as palavras que faltavam.
Treinando um Classificador Simples
Vimos que os estados ocultos são um pouco diferentes entre as emoções, embora por
vários deles não há um limite óbvio. Vamos usar esses estados ocultos para treinar um simples
regressor logístico com Scikit-Learn! Treinar um modelo tão simples é rápido e não requer um
GPU.

Observando a precisão, pode parecer que nosso modelo é um pouco melhor do que aleatório, mas
como estamos lidando com um conjunto de dados multiclasse não balanceado, isso é significativamente melhor do que
aleatório. Podemos ter uma ideia melhor se nosso modelo é bom comparando-o com um
linha de base simples. No Scikit-Learn existe um DummyClassifier que pode ser usado para construir um
classificador com heurísticas simples, como sempre escolher a classe majoritária ou sempre desenhar uma classe aleatória.


que produz uma precisão de cerca de 35%. Portanto, nosso classificador simples com incorporações BERT é
significativamente melhor do que nossa linha de base. Podemos investigar melhor o desempenho do modelo
olhando para a matriz de confusão do classificador, que nos diz a relação entre o
rótulos verdadeiros e previstos.

Podemos ver que a raiva e o medo são mais frequentemente confundidos com tristeza, o que concorda com
a observação que fizemos ao visualizar os embeddings. Também amor e surpresa são
frequentemente confundido com alegria.
Para obter uma imagem ainda melhor do desempenho da classificação, podemos imprimir o Scikit-Learn's
relatório de classificação e observe a precisão, revocação e F-score para cada classe:

Na próxima seção, exploraremos a abordagem de ajuste fino que leva a resultados superiores.
desempenho da classificação. No entanto, é importante notar que fazer isso requer muito mais
recursos computacionais, como GPUs, que podem não estar disponíveis em sua empresa. Em casos
assim, uma abordagem baseada em recursos pode ser um bom compromisso entre fazer o tradicional
aprendizado de máquina e aprendizado profundo.

Transformadores de ajuste fino
Vamos agora explorar o que é necessário para ajustar um Transformer de ponta a ponta. Com o ajuste fino
abordagem, não usamos os estados ocultos como recursos fixos, mas, em vez disso, os treinamos conforme mostrado em
Figura 2-6. Isso requer que a cabeça de classificação seja diferenciável, e é por isso que esse método
geralmente usa uma rede neural para classificação. Como retreinamos todos os parâmetros do DistilBERT,
essa abordagem requer muito mais computação do que a abordagem de extração de recursos e normalmente
requer uma GPU.

Como treinamos os estados ocultos que servem como entradas para o modelo de classificação, também evitamos
o problema de trabalhar com dados que podem não ser adequados para a tarefa de classificação. Em vez de,
os estados ocultos iniciais se adaptam durante o treinamento para diminuir a perda do modelo e, assim, aumentar sua
desempenho. Se a computação necessária estiver disponível, esse método é comumente escolhido em vez do
abordagem baseada em recursos, uma vez que geralmente supera isso.
Usaremos a API Trainer da Transformers para simplificar o loop de treinamento - vamos ver
os ingredientes que precisamos para preparar um!

A primeira coisa que precisamos é de um modelo DistilBERT pré-treinado como o que usamos na abordagem baseada em recursos. A única pequena modificação é que usamos o
Modelo AutoModelForSequenceClassification em vez de AutoModel. O
A diferença é que o modelo AutoModelForSequenceClassification tem um
cabeça de classificação sobre as saídas do modelo que podem ser facilmente treinadas com o modelo base.
Só precisamos especificar quantos rótulos o modelo tem para prever (seis no nosso caso), pois isso
dita o número de saídas que o chefe de classificação tem.

Você provavelmente verá um aviso de que algumas partes dos modelos são inicializadas aleatoriamente. Isso é
normal, pois o chefe de classificação ainda não foi treinado.
Pré-processar os Tweets
Além da tokenização, também precisamos definir o formato das colunas para
arch.Tensor. Isso nos permite treinar o modelo sem precisar mudar para frente e para trás
entre listas, arrays e tensores. Com conjuntos de dados, podemos usar a função set_format para
alterar o tipo de dados das colunas que desejamos manter, descartando todo o resto.

Além disso, definimos algumas métricas que são monitoradas durante o treinamento. Isso pode ser qualquer
função que recebe um objeto de previsão, que contém as previsões do modelo, bem como o correto
rótulos e retorna um dicionário com valores de métrica escalar. Vamos monitorar o F-score e o
precisão do modelo.

Treinando o Modelo

Aqui também definimos o tamanho do lote, taxa de aprendizado, número de épocas e também especificamos para carregar o
melhor modelo no final da corrida de treinamento. Com este ingrediente final, podemos instanciar e ajustar nosso modelo com o Trainer

Época

Perda de treinamento

Perda de validação

Precisão

Observando os logs, podemos ver que nosso modelo tem uma pontuação F no conjunto de validação de cerca de
92% - esta é uma melhoria significativa em relação à abordagem baseada em recursos! Também podemos ver que
o melhor modelo foi salvo executando o método de avaliação:

Vamos dar uma olhada mais detalhada nas métricas de treinamento calculando a matriz de confusão.
Visualize a Matriz de Confusão
Para visualizar a matriz de confusão, primeiro precisamos obter as previsões no conjunto de validação. O
função de previsão da classe Trainer retorna vários objetos úteis que podemos usar para
avaliação.

Ele também contém as previsões brutas para cada classe. Decodificamos as previsões avidamente com um
argmax. Isso produz o rótulo previsto e tem o mesmo formato dos rótulos retornados pelo
Modelos Scikit-Learn na abordagem baseada em recursos.


Com as previsões, podemos traçar a matriz de confusão novamente:

Podemos ver que as previsões estão muito mais próximas da matriz de confusão diagonal ideal. O
categoria de amor ainda é frequentemente confundida com alegria que parece natural. Além disso, surpresa
e o medo muitas vezes são confundidos e a surpresa também é frequentemente confundida com alegria.
No geral, o desempenho do modelo parece muito bom.
Além disso, observar o relatório de classificação revela que o modelo também está tendo um desempenho muito melhor
para classes minoritárias como surpresa.

lembrar

pontuação f1

apoiar

tristeza
alegria
amor
raiva
temer
surpresa

Fazendo previsões
Também podemos usar o modelo ajustado para fazer previsões sobre novos tweets. Primeiro, precisamos
tokenize o texto, passe o tensor pelo modelo e extraia os logits.

As previsões do modelo não são normalizadas, o que significa que não são uma distribuição de probabilidade
mas as saídas brutas antes da camada softmax.


Podemos facilmente transformar as previsões em uma distribuição de probabilidade aplicando uma função softmax para
eles. Como temos um tamanho de lote de 1, podemos nos livrar da primeira dimensão e converter o
tensor para uma matriz NumPy para processamento na CPU.

Podemos ver que as probabilidades agora estão devidamente normalizadas observando a soma que
soma 1.

Erro de análise
Antes de prosseguir, devemos investigar um pouco mais a previsão do nosso modelo. Um simples, mas
ferramenta poderosa é classificar as amostras de validação pela perda do modelo. Ao passar a etiqueta durante
o passe para frente, a perda é calculada automaticamente e retornada. Abaixo está uma função que
retorna a perda junto com o rótulo previsto.
seguindo.

Rótulos errados
Todo processo que adiciona rótulos aos dados pode ser falho; anotadores podem cometer erros ou
discordar, inferir rótulos de outros recursos pode falhar. Se fosse fácil automaticamente
anotar dados, então não precisaríamos de um modelo para fazer isso. Assim, é normal que existam
alguns exemplos rotulados erroneamente. Com essa abordagem, podemos encontrá-los e corrigi-los rapidamente.
Peculiaridades do conjunto de dados
Conjuntos de dados no mundo real são sempre um pouco confusos. Ao trabalhar com texto pode acontecer
que existem alguns caracteres especiais ou strings nas entradas que confundem o modelo.
A inspeção das previsões mais fracas do modelo pode ajudar a identificar tais recursos e limpar o
dados ou injetar exemplos semelhantes pode tornar o modelo mais robusto.
Vamos primeiro dar uma olhada nas amostras de dados com as maiores perdas.

sou preguiçoso, meus personagens se enquadram nas categorias de presunçoso e/ou blas alegria
pessoas e seus contrastes pessoas que se sentem incomodadas por
pessoas presunçosas e ou blasfemas

temer

eu me chamei de pró vida e votei no perry sem saber
esta informação eu me sentiria traído, mas, além disso, eu
Sinto que traí Deus ao apoiar um homem que
determinou uma vacina de apenas um ano de idade para as meninas que colocam
em perigo para apoiar financeiramente pessoas próximas a ele

alegria

tristeza

Eu também me lembro de sentir como se todos os olhos estivessem em mim o tempo todo
e não de uma forma glamorosa e eu odiei

alegria

raiva

Estou meio envergonhado por me sentir assim
porque o treinamento da minha mãe foi uma definição maravilhosa
parte da minha própria vida e eu amei e ainda amo

amor

tristeza

sinto-me mal por não ter cumprido o meu compromisso de trazer
rosquinhas para os fiéis na igreja católica sagrada família em
columbus ohio

amor

tristeza

acho que me sinto traída porque o admirava muito e de alegria
alguém fazer isso com sua esposa e filhos vai além do
pálido

tristeza

quando notei duas aranhas correndo no chão em diferentes
instruções

raiva

temer

Eu deixei você matá-lo agora, mas na verdade não estou sentindo
assustadoramente bem hoje

alegria

temer

eu me sinto como o garotinho idiota e nerd sentado em seu quintal toda alegria
sozinho ouvindo e observando através da cerca para o pequeno
garoto popular tendo sua festa de aniversário com todos os seus amigos legais
que você sempre desejou que fosse seu

Podemos ver claramente que o modelo previu alguns dos rótulos errados. Por outro lado
parece que existem alguns exemplos sem classe clara que podem ser mal rotulados
ou exigir uma nova classe completamente. Em particular, a alegria parece ser mal rotulada várias vezes. Com
Com essas informações, podemos refinar o conjunto de dados, o que geralmente pode levar a um desempenho igual ou superior
ganho como tendo mais dados ou modelos maiores!
Ao olhar para as amostras com as menores perdas, observamos que o modelo parece ser
mais confiante ao prever a classe de tristeza. Os modelos de aprendizado profundo são excepcionalmente
bom em encontrar e explorar atalhos para chegar a uma previsão. Uma famosa analogia para ilustrar

Este é o cavalo alemão Hans do início do século XX. Hans foi uma grande sensação desde que ele
aparentemente era capaz de fazer aritmética simples, como somar dois números tocando no resultado;
uma habilidade que lhe rendeu o apelido de Clever Hans. Estudos posteriores revelaram que Hans era
realmente não é capaz de fazer aritmética, mas pode ler o rosto do questionador e determinar com base
na expressão facial quando alcançou o resultado correto.

Os modelos de aprendizado profundo tendem a encontrar explorações semelhantes se os recursos permitirem. Imagine que construímos um
modelo de sentimento para analisar o feedback do cliente. Suponhamos que acidentalmente o número de
as estrelas que o cliente deu também estão incluídas no texto. Em vez de realmente analisar o texto, o
modelo pode simplesmente aprender a contar as estrelas na revisão. Quando implantamos esse modelo em
produção e não tiver mais acesso a essas informações, terá um desempenho ruim e, portanto,
queremos evitar tais situações. Por isso vale a pena investir tempo olhando para o
exemplos nos quais o modelo está mais confiante, para que possamos ter certeza de que o modelo
não explora certas características do texto.

Agora sabemos que a alegria às vezes é mal rotulada e que o modelo está mais confiante sobre
dando ao rótulo tristeza. Com essas informações, podemos fazer melhorias direcionadas ao nosso conjunto de dados e também ficar de olho na classe, o modelo parece ser muito
confiante sobre. O último
A primeira etapa antes de servir o modelo treinado é salvá-lo para uso posterior. A biblioteca do transformador
permite fazer isso em algumas etapas que mostramos na próxima seção.
Salvando o Modelo
Finalmente, queremos salvar o modelo para que possamos reutilizá-lo em outra sessão ou mais tarde se quisermos
colocá-lo em produção. Podemos salvar o modelo junto com o tokenizer correto na mesma pasta

A comunidade de PNL se beneficia muito com o compartilhamento de modelos pré-treinados e ajustados, e
todos podem compartilhar seus modelos com outras pessoas por meio do Hugging Face Model Hub. Através de
Hub, todos os modelos gerados pela comunidade podem ser baixados da mesma forma que baixamos o
Modelo DistilBert.

Depois de fazer login com suas credenciais do Model Hub, a próxima etapa é criar um Git
repositório para armazenar seu modelo, tokenizer e quaisquer outros arquivos de configuração:
repositório de transformadores-cli criar distilbert-emotion

Isso cria um repositório no Model Hub que pode ser clonado e versionado como qualquer outro
Repositório Git. A única sutileza é que o Model Hub usa Git Large File Storage para modelo
controle de versão, portanto, certifique-se de instalá-lo antes de clonar o repositório:

Agora salvamos nosso primeiro modelo para mais tarde. Este não é o fim da jornada, mas apenas o primeiro
iteração. Construir modelos de alto desempenho requer muitas iterações e análises minuciosas e no
Na próxima seção, listamos alguns pontos para obter mais ganhos de desempenho.

Melhorias adicionais
Há uma série de coisas que poderíamos tentar para melhorar o modelo baseado em recursos que treinamos isso
capítulo. Por exemplo, como os estados ocultos são apenas recursos do modelo, podemos incluir
recursos adicionais ou manipular os existentes. As etapas a seguir podem render mais
melhora e seriam bons exercícios:
Abordar o desequilíbrio de classes aumentando ou diminuindo a amostragem das classes minoritárias ou majoritárias
respectivamente. Alternativamente, o desequilíbrio também pode ser tratado na classificação
modelo ponderando as classes.
Adicione mais incorporações de diferentes modelos. Existem muitos modelos do tipo BERT que
têm um estado oculto ou saída que poderíamos usar como ALBERT, GPT-2 ou ELMo. Você
poderia concatenar a incorporação do tweet de cada modelo para criar uma grande entrada
recurso.
Aplique a engenharia de recursos tradicional. Além de usar os embeddings do Transformer
modelos, também poderíamos adicionar recursos como a duração do tweet ou se determinados
emojis ou hashtags estão presentes.
Embora o desempenho do modelo ajustado já pareça promissor, ainda há alguns
coisas que você pode tentar melhorar: - Usamos valores padrão para os hiperparâmetros, como
taxa de aprendizado, queda de peso e etapas de aquecimento, que funcionam bem para classificação padrão
tarefas. No entanto, o modelo ainda pode ser melhorado com o ajuste deles e consulte o Capítulo 5, onde
usamos Optuna para ajustar sistematicamente os hiperparâmetros. - Modelos destilados são ótimos para sua
desempenho com recursos computacionais limitados. Para algumas aplicações (por exemplo, baseado em lote
implementações), a eficiência pode não ser a principal preocupação, então você pode tentar melhorar o
desempenho usando o modelo completo. Para espremer até a última gota de desempenho, você também pode
tente combinar vários modelos. - Descobrimos que alguns rótulos podem estar errados, o que é
às vezes referido como ruído de rótulo. Voltar ao conjunto de dados e limpar os rótulos é uma
passo essencial ao desenvolver aplicações de PNL. - Se o ruído da etiqueta for uma preocupação, você também pode
pense em aplicar a suavização de rótulo.6 Suavizar os rótulos de destino garante que o modelo
não fica excessivamente confiante e traça limites de decisão mais claros. A suavização de etiquetas já está
embutido no Trainer e pode ser controlado por meio do argumento label_smoothing_factor.

Conclusão
Parabéns, agora você sabe como treinar um modelo Transformer para classificar as emoções em
tweets! Vimos duas abordagens complementares usando recursos e ajustes finos e
investigaram seus pontos fortes e fracos. Melhorar qualquer um dos modelos é uma tarefa em aberto
esforço e listamos vários caminhos para melhorar ainda mais o modelo e o conjunto de dados.
No entanto, este é apenas o primeiro passo para a construção de um aplicativo do mundo real com Transformers
então para onde a partir daqui? Aqui está uma lista de desafios que você provavelmente enfrentará ao longo do caminho
que abordamos neste livro:
Meu chefe quer meu modelo em produção ontem! - No próximo capítulo mostraremos
como empacotar nosso modelo como um aplicativo da web que você pode implantar e compartilhar com
seus colegas.
Meus usuários querem previsões mais rápidas! - Já vimos neste capítulo que o DistilBERT
é uma abordagem para este problema e em capítulos posteriores vamos nos aprofundar em como
a destilação realmente funciona, junto com outros truques para acelerar o seu Transformer
modelos.
O seu modelo também pode fazer X? - Como mencionamos neste capítulo, os Transformers são
extremamente versátil e, no restante do livro, exploraremos uma série de tarefas
como resposta a perguntas e reconhecimento de entidade nomeada, todos usando o mesmo
arquitetura.
Nenhum dos meus textos está em inglês! - Acontece que os Transformers também vêm em um
variedade multilíngue e os usaremos para lidar com tarefas em vários idiomas ao mesmo tempo.
Não tenho rótulos! - O aprendizado de transferência permite que você ajuste alguns rótulos e
mostraremos como eles podem ser usados ​​para anotar dados não rotulados com eficiência.
No próximo capítulo, veremos como os Transformers podem ser usados ​​para recuperar informações de
grandes corpora e encontrar respostas para perguntas específicas.
