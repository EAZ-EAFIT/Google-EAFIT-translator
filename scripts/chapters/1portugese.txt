Capítulo 1. Olá Transformers
UMA OBSERVAÇÃO PARA LEITORES DE LANÇAMENTOS ANTECIPADOS
Com os e-books de lançamento antecipado, você obtém os livros em sua forma mais antiga - a versão bruta e
conteúdo não editado enquanto eles escrevem - para que você possa aproveitar essas tecnologias por muito tempo
antes do lançamento oficial desses títulos.
Este será o 1º capítulo do livro final. Observe que o repositório do GitHub será feito
ativo mais tarde.
Se você tiver comentários sobre como podemos melhorar o conteúdo e/ou exemplos neste
livro, ou se você notar falta de material neste capítulo, entre em contato com o editor em
mpotter@oreilly. com.
Desde sua introdução em 2017, os transformadores se tornaram o padrão de fato para lidar com uma
ampla gama de tarefas de processamento de linguagem natural (PNL) tanto na academia quanto na indústria. Sem
percebendo isso, você provavelmente interagiu com um transformador hoje: o Google agora usa o BERT para
aprimorar seu mecanismo de pesquisa, compreendendo melhor as consultas de pesquisa dos usuários. Da mesma forma, o GPT
família de transformadores da OpenAI repetidamente ganhou as manchetes na mídia convencional para
sua capacidade de gerar textos e imagens semelhantes aos humanos. Esses transformadores agora alimentam
aplicativos como o Copilot do GitHub, que, conforme mostrado na Figura 1-1, pode converter um comentário em
código-fonte que treina automaticamente uma rede neural para você!

Figura 1-1. Um exemplo do GitHub Copilot onde dada uma breve descrição da tarefa, o aplicativo fornece uma sugestão
para toda a função (mostrada em cinza), completa com comentários úteis.

Então, o que há nos transformadores que mudaram o campo quase da noite para o dia? Como muitos grandes
descobertas científicas, foi o culminar de várias ideias como atenção, transferência
aprendizado e ampliação de redes neurais que estavam se infiltrando na comunidade de pesquisa em
A Hora.
Mas um novo método sofisticado por si só não é suficiente para ganhar força na indústria - ele também exige
ferramentas para torná-lo acessível. A biblioteca Hugging Face Transformers e seus arredores

O ecossistema respondeu a essa chamada ajudando os profissionais a usar, treinar e compartilhar modelos com facilidade, o que
acelerou muito a adoção de transformadores na indústria. A biblioteca é hoje utilizada por
mais de 1.000 empresas para operar transformadores em produção, e ao longo deste livro vamos orientar
você sobre como treinar e otimizar esses modelos para aplicações práticas.
Neste capítulo, apresentaremos os conceitos centrais que fundamentam a difusão do
transformadores, faça um tour por algumas das tarefas em que eles se destacam e conclua com uma olhada no
Ecossistema de ferramentas e bibliotecas do Hugging Face. Vamos começar nossa jornada transformadora com um breve
visão histórica.

A História da Origem dos Transformers
Em 2017, pesquisadores do Google publicaram um artigo1 que propunha uma nova rede neural
arquitetura para modelagem de sequência. Apelidada de Transformer, essa arquitetura superou
redes neurais recorrentes (RNNs) em tarefas de tradução automática, tanto em termos de tradução
qualidade e custo de treinamento.
Paralelamente, um método eficaz de aprendizado por transferência chamado ULMFiT2 mostrou que o pré-treinamento
Redes de memória de longo prazo (LSTM) com um objetivo de modelagem de linguagem em um
corpus grande e diversificado e, em seguida, o ajuste fino em uma tarefa de destino poderia produzir um texto robusto
classificadores com poucos dados rotulados.
Esses avanços foram os catalisadores para dois dos transformadores mais conhecidos atualmente: GPT
e BERT. Ao combinar a arquitetura do Transformer com o pré-treinamento do modelo de linguagem, esses
modelos eliminaram a necessidade de treinar arquiteturas específicas de tarefas do zero e quebraram quase
cada benchmark em PNL por uma margem significativa. Desde o lançamento do GPT e do BERT, um
surgiu um verdadeiro zoológico de modelos de transformadores e uma linha do tempo dos eventos recentes é mostrada em
Figura 1-2.

Figura 1-2. A linha do tempo dos transformadores.

Mas estamos ficando à frente de nós mesmos. Para entender o que há de novo nessa abordagem
combinando conjuntos de dados muito grandes e uma nova arquitetura:
A estrutura do codificador-decodificador

Mecanismos de atenção
Transferência de aprendizagem
Vamos começar examinando a estrutura do codificador-decodificador e as arquiteturas que precederam
surgimento dos transformadores.
OBSERVAÇÃO
Neste livro, usaremos o nome próprio “Transformer” (singular e com primeira letra maiúscula) para nos referirmos ao
arquitetura de rede neural original que foi introduzida no agora famoso artigo Attention is All You Need. Para
a classe geral de modelos baseados em Transformer, usaremos "transformers" (plural e com inicial não maiúscula
letra) e para a biblioteca Hugging Face usaremos a abreviatura “Transformers” (plural e com inicial maiúscula
carta). Espero que isso não seja muito confuso!

A Estrutura do Codificador-Decodificador
Antes dos transformadores, os LSTMs eram o estado da arte em PNL. Essas arquiteturas contêm um
ciclo ou loop de feedback nas conexões de rede que permite que as informações se propaguem de
passo a passo, tornando-os ideais para modelar dados sequenciais como a linguagem. Como
ilustrado na imagem à esquerda da Figura 1-3, um RNN recebe alguma entrada x (que pode ser um
palavra ou caractere), o alimenta através da rede A e gera um valor chamado h chamado de
estado oculto. Ao mesmo tempo, o modelo alimenta algumas informações de volta para si mesmo através do
loop de feedback que pode ser usado na próxima etapa com a entrada x . Isso pode ser mais claramente
visto se “desenrolarmos” o loop conforme mostrado no lado direito da Figura 1-3, onde a cada passo o
RNN passa informações sobre seu estado para a próxima operação na sequência. Isso permite um
RNN para acompanhar as informações das etapas anteriores e usá-las para suas previsões de saída.


TODO: Redesenhe ou faça referência a Chris Olah
Essas arquiteturas foram (e continuam sendo) amplamente utilizadas para tarefas em NLP, processamento de fala,
e séries temporais, e você pode encontrar uma exposição maravilhosa de suas capacidades em Andrej
Postagem no blog de Karpathy, A eficácia irracional das redes neurais recorrentes.
Uma área em que as RNNs desempenharam um papel importante foi no desenvolvimento da tradução automática
sistemas, onde o objetivo é mapear uma sequência de palavras de um idioma para outro. Esse
tipo de tarefa geralmente é abordada com um codificador-decodificador ou arquitetura sequência a sequência,3
que é adequado para situações em que a entrada e a saída são sequências arbitrárias
comprimento. Como o nome sugere, o trabalho do codificador é codificar as informações da entrada
sequência em uma representação numérica que geralmente é chamada de último estado oculto. Este estado é
então passado para o decodificador, que gera a sequência de saída.

Em geral, os componentes do codificador e do decodificador podem ser qualquer tipo de arquitetura de rede neural
que é adequado para modelar sequências, e esse processo é ilustrado para um par de RNNs em
Figura 1-4, onde a frase em inglês “Transformers are great!” é convertido para um estado oculto
vetor que é então decodificado para produzir a tradução alemã “Transformer sind grossartig!”.
Nesta figura, as caixas sombreadas representam as células RNN desenroladas onde as linhas verticais são
os circuitos de feedback. Os tokens são alimentados sequencialmente através do modelo e os tokens de saída são
igualmente criados sequencialmente de cima para baixo.


Embora elegante em sua simplicidade, uma fraqueza dessa arquitetura é que o final oculto
estado do codificador cria um gargalo de informação: ele tem que capturar o significado do
sequência de entrada inteira porque é tudo o que o decodificador tem acesso ao gerar a saída.
Isso é especialmente desafiador para sequências longas em que as informações no início da sequência
podem ser perdidos no processo de criação de uma representação única e fixa.
Felizmente, existe uma saída para esse gargalo, permitindo que o decodificador tenha acesso a todos os
